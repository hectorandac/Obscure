\section{Methodology} 

Our study introduces an innovative approach to optimize object detection within the constraints of limited computational resources, specifically through the development of the Gated YOLO architecture. This methodology leverages a dynamic gating mechanism, a novel adaptation within the established YOLO framework, to enhance the model's efficiency and adaptability by selectively processing only the neural pathways relevant to the observed scene. The primary focus of this section is to delineate the systematic approach employed in the design, implementation, and validation of this architecture. We outline the steps taken to integrate dynamic gating with the YOLO architecture, including the development and deployment of the Gater Network, the modifications applied to the YOLO architecture for accommodating gating mechanisms, and the analytical methods used to assess the model's performance in varied scene-specific contexts. The subsequent paragraphs will detail each component of our methodology, elucidating the technical strategies employed to achieve a balance between computational efficiency and detection accuracy in resource-constrained environments.

\subsection{Gater Network}
The cornerstone of our Gated YOLO model is the Gater Network, a novel component designed to enhance the model's computational efficiency by dynamically modulating the activation of neural pathways based on the specific features of the input scene. This is achieved by the generation of gates influenced by the scene salient characteristics. Utilizing the principles of conditional computation, the Gater Network strategically deactivates certain parts of the neural network that are deemed irrelevant for a given scene, thereby reducing unnecessary computational overhead with minimal compromise to the model's object detection capabilities.

The Gater Network operates in two main phases: gate generation during training and gate application during inference. During the training phase, the network learns to identify and generate a set of binary gates based on the distinguishing features of the input images. These gates serve as indicators for activating or deactivating specific channels within the YOLO architecture, depending on their relevance to the task at hand. The process leverages Improved Semantic Hashing, a technique inspired by the work of Kaiser and Bengio~\cite{kaiser2018discrete} and further explored by Chen et al.~\cite{chen2019you}, to ensure that the gate generation is both efficient and effective.

\paragraph{Gate Generation and Application.} In practice, the Gater Network employs a specialized architecture, typically based on a lightweight convolutional neural network (CNN) like the ResNet family of light weight variations~\cite{he2016deep}, to process input images and extract relevant features. These features are then passed through a series of fully connected layers, culminating in the generation of binary gates. Each gate corresponds to a specific channel or filter in the subsequent layers of the YOLO architecture, dictating whether it should be activated (1) or deactivated (0) based on the input scene's characteristics. An overview on how the GaterNet generates these gates can be seen in Fig.\ref{fig:gaternet_architecture}.

\begin{figure}[ht]
    \centering
    \includesvg[width=\textwidth]{./figures/gaternet}
    \caption{Illustration of the GaterNet architecture. The GaterNet extract features using ResNet-18 and process the output to generate a set of binary gates.}
    \label{fig:gaternet_architecture}
    \end{figure}

During inference, the pre-determined gates are applied to the YOLO architecture, enabling the model to focus its computational resources only on the parts of the network that are essential for detecting objects in the current scene. This selective processing significantly enhances the model's efficiency, particularly in scenarios where computational resources are limited, such as edge devices.

\paragraph{Dynamic Adaptation to Scene Features.} One of the key advantages of the Gater Network is its ability to dynamically adapt to various scenes without the need for manual tuning. By analyzing the input scene and applying the appropriate gates, the network ensures that only the most relevant features are processed. This adaptability is crucial for applications such as surveillance and traffic monitoring, where the scene's characteristics may vary significantly but remain consistent over time.

In summary, the Gater Network introduces a significant advancement in the YOLO architecture by incorporating a dynamic gating mechanism that intelligently deactivates filters based on the unique features of each input scene. This approach not only enhances computational efficiency but also maintains high detection accuracy, demonstrating the potential of selective gating in improving real-time object detection applications.

\subsubsection{Feature Extraction}
The feature extraction process within the Gater Network plays a pivotal role in our Gated YOLO model, laying the foundation for the subsequent gating mechanism by identifying the critical features from the input images. This process can make use of any DNN like the ResNet family. In our experiments we defaulted to ResNet-18~\cite{he2016deep} for its renowned efficiency and effectiveness in capturing salient features from images with minimal computational resources. Additionally we analyzed the effect of using more complex variations to enhance the GaterNet representations capabilities, the result of which can be reviewed in Table \ref{tab:feature_extractor}, within the ablation studies section.

\paragraph{ResNet-18 for Efficient Feature Representation.} ResNet-18 is chosen for its shallow architecture compared to deeper variants, striking an optimal balance between computational efficiency and the ability to extract rich, discriminative features. This balance is crucial for our model's application in resource-constrained environments, where maintaining high accuracy without excessive computational burden is essential. The feature extraction is formalized as follows in equation~(\ref{eq:feature_flatten}):

\begin{equation}
    F_{\text{extract}}(x) = \text{Flatten}(\text{AdaptivePool}(f_{\text{net}}(x))),
    \label{eq:feature_flatten}
\end{equation}

\noindent{}where \(x \in \mathbb{R}^{c_0 \times h_0 \times w_0}\) represents the input image, and \(f_{\text{net}}(x)\) denotes the feature representation extracted by the ResNet-18 network. The \(AdaptivePool\) operation ensures that the output from \(f_{\text{net}}\) is standardized, facilitating uniformity across different input dimensions. The final feature vector \(F_{\text{extract}}(x)\) is obtained by flattening the pooled features, making it suitable for further processing by the fully connected layers leading to gate generation.

\paragraph{Adaptive Pooling for Dimensionality Reduction.} Adaptive pooling plays a crucial role in our feature extraction process by dynamically adjusting the size of the feature maps to a fixed dimension, thereby enabling a consistent input size for the fully connected layers regardless of the original input image size. This step is critical for ensuring that the feature extraction process remains efficient and scalable across varying image dimensions.

\paragraph{Feature Flattening for Gate Generation.} After adaptive pooling, the feature map undergoes a flattening operation, transforming it into a one-dimensional vector suitable for analysis by the subsequent layers responsible for gate generation. This flattened feature vector encapsulates the essential information required for determining the relevance of specific neural pathways in the YOLO architecture, forming the basis for the dynamic gating mechanism.

\subsubsection{Binary Gates}
Binary gates within the Gater Network are pivotal for modulating the activity of neural pathways in the YOLO architecture, enabling selective processing based on the input scene's characteristics. These gates are generated through a series of operations that map the high-dimensional feature vector obtained from the feature extraction phase to a binary vector, where each element corresponds to a specific channel in the YOLO architecture. The generation and application of these binary gates are fundamentally rooted in the concept of Improved Semantic Hashing~\cite{kaiser2018discrete,chen2019you}, which ensures the differentiability of the gating process during training while maintaining binary decisions during inference. As default for our implementation we sampled noise from the Gaussian distribution. Additionally, experiments were performed with Gumbel distribution to exploit the Gumbel-Softmax approach, the findings for this venture can be validated on Table \ref{tab:noise_distribution} within the ablation studies section.

\paragraph{Mapping to Binary Space.} The process begins with mapping the extracted features to the binary space. This mapping is achieved through a dual-layer architecture comprising two fully connected layers that introduce a bottleneck layer to efficiently manage the parameter space while ensuring the representational capacity:

\begin{equation}
f_0 = \text{ReLU}(\text{BatchNorm}(\text{FC1}(f))), \quad \text{and}
\label{eq:bottleneck_mapping}
\end{equation}

\begin{equation}
g_0 = \text{FC2}(f_0).
\label{eq:binary_mapping}
\end{equation}

\noindent{}where in equation~(\ref{eq:bottleneck_mapping}), \(f\) denotes the flattened feature vector, and \(f_0\) represents the intermediate representation at the bottleneck. \(FC1\) and \(FC2\) are the two fully connected layers, with ReLU activation and Batch Normalization applied after the first layer to enhance training stability and non-linearity. Equation~(\ref{eq:binary_mapping}) maps \(f_0\) to \(g_0\), the pre-activation gate vector, poised for binary conversion.

\paragraph{Improved Semantic Hashing.} The cornerstone of our binary gate generation process is the Improved Semantic Hashing technique, which allows the network to generate binary gates in a differentiable manner during training. This adaptability is crucial for integrating the gating mechanism within the end-to-end training process of the YOLO architecture. The method involves adding a noise component to the pre-activation gate vector and applying a sigmoid function to obtain a soft binary gate:

\begin{equation}
g_{\text{noisy}} = g_0 + \epsilon, \quad \text{and}
\label{eq:noisy_g}
\end{equation}

\begin{equation}
g_{\alpha}(i) = \text{clamp}(1.2 \times \sigma(g_{\text{noisy}}(i)) - 0.1, 0, 1),
\label{eq:soft_binary_gate}
\end{equation}

\noindent{}where \(\epsilon\) represents a noise vector sampled from a Gaussian distribution in equation~(\ref{eq:noisy_g}). \(g_{\text{noisy}}\) denotes the noisy gate vector, and \(g_{\alpha}\) is the soft binary gate vector, with each element being clamped between 0 and 1 to ensure binary-like behavior as seen in equation~(\ref{eq:soft_binary_gate}). This approach facilitates the backpropagation of gradients during training, allowing for the optimization of the gating mechanism.

\paragraph{Binary Decision Making.} During inference, the soft binary gates \(g_{\alpha}\) are converted into hard binary decisions \(g_{\beta}\), which directly control the activation of corresponding channels in the YOLO architecture, as shown in equation~(\ref{eq:hard_binary_decision}):

\begin{equation}
g_{\beta}(i) = 
\begin{cases}
1, & \text{if } g_{\alpha}(i) \geq \text{Gating threshold},\\
0, & \text{otherwise}.
\end{cases}
\label{eq:hard_binary_decision}
\end{equation}

This binary decision-making process ensures that only the relevant features contributing to the object detection task are processed, thereby enhancing the model's computational efficiency without compromising its detection performance.

In essence, the Binary Gates subsection outlines the sophisticated methodology employed to generate and apply binary gates within the Gater Network, leveraging Improved Semantic Hashing to marry the model's need for differentiability during training with the necessity for discrete decision-making during inference. This delicate balance enables the Gated YOLO model to dynamically adapt to various scene features, optimizing computational resources while slightly impacting the accuracy in object detection tasks.

\subsubsection{Loss Function}
The loss function of our Gated YOLO model is meticulously designed to optimize both object detection performance and the efficiency of the gating mechanism. It amalgamates multiple components to guide the training process towards achieving high accuracy in object detection while ensuring computational efficiency through effective gate generation. The overall loss function is formulated as follows in equation~(\ref{eq:total_loss}):

\begin{equation}
L = \alpha_{\text{cls}} L_{\text{cls}} + \alpha_{\text{iou}} L_{\text{iou}} + \alpha_{\text{dfl}} L_{\text{dfl}} + \lambda \cdot L_{\text{gate}},
\label{eq:total_loss}
\end{equation}

\noindent{}where \(L_{\text{cls}}\), \(L_{\text{iou}}\), and \(L_{\text{dfl}}\) represent the classification loss, Intersection over Union (IoU) loss, and distance-IoU loss, respectively. These components are weighted by their respective coefficients \(\alpha_{\text{cls}}\), \(\alpha_{\text{iou}}\), and \(\alpha_{\text{dfl}}\), ensuring a balanced contribution to the overall loss. \(L_{\text{gate}}\) signifies the gating loss, which encourages the sparsity of the gating mechanism, and \(\lambda\) is the regularization coefficient controlling its influence on the total loss.

\paragraph{Classification and IoU Losses.} The classification loss \(L_{\text{cls}}\) and IoU loss \(L_{\text{iou}}\) are fundamental to object detection models, ensuring accurate classification and localization of objects within the scene~\cite{li2023yolov6,li2022yolov6}. The distance-IoU loss \(L_{\text{dfl}}\) further refines the bounding box predictions, enhancing the precision of object localization.

\paragraph{Gating Loss.} The gating loss \(L_{\text{gate}}\) is pivotal in training the Gater Network, promoting the generation of efficient and effective gates. It is designed to encourage the model to minimize the number of active gates, thereby reducing the computational load during inference. The gating loss is formulated as an \(L_1\) regularization term over the gate vector \(g\), encouraging sparsity. This loss can be expressed in equation~(\ref{eq:gate_loss}) as follows:

\begin{equation}
L_{\text{gate}} = \frac{1}{c} \| g \|_1 = \frac{1}{c} \sum_{i} | g_i |,
\label{eq:gate_loss}
\end{equation}

\noindent{}where \(c\) is the total number of gates, and \(g_i\) represents the individual gate values. By penalizing the sum of the absolute values of the gate vector, the model is encouraged to deactivate unnecessary channels, thereby streamlining the network for increased efficiency.

\paragraph{Balancing Detection Performance and Efficiency.} The coefficients \(\alpha_{\text{cls}}\), \(\alpha_{\text{iou}}\), \(\alpha_{\text{dfl}}\), and the regularization term \(\lambda\) play crucial roles in balancing the model's object detection performance with the efficiency of the gating mechanism. By adjusting these parameters, we can fine-tune the model to prioritize either detection accuracy or computational efficiency, depending on the application requirements.

Additionally, the gating mechanism's threshold, represented as \texttt{gtg\_threshold}, influences the binary decision process in the gating mechanism. The gating loss \(L_{\text{gate}}\) is dynamically scaled using the \texttt{gtg\_decay} parameter, which adjusts the weight of the gating loss over epochs to balance the trade-off between gate sparsity and detection performance.

In conclusion, the loss function of the Gated YOLO model is a comprehensive formulation that encompasses the dual objectives of maintaining high accuracy in object detection and optimizing computational efficiency through an effective gating mechanism. This strategic combination of loss components ensures that the model can be effectively trained to meet the demands of real-world object detection tasks in resource-constrained environments.

\subsection{YOLO Architecture}

In the design of our Gated YOLO model, we leverage the strengths of the YOLOv6 architecture, as proposed by Li et al.~\cite{li2023yolov6,li2022yolov6}, renowned for its exceptional real-time performance and optimal utilization of hardware resources. The YOLOv6 framework serves as the foundational backbone, onto which we have integrated the GaterNet that introduces a dynamic gating mechanism. This integration enables our model to perform selective feature processing, substantially improving computational efficiency without sacrificing detection accuracy. The following figure illustrates the modified YOLO architecture augmented with the GaterNet functionality:

\begin{figure}[ht]
    \centering
    \includesvg[width=\textwidth]{./figures/yolov6_architecture}
    \caption{Illustration of the YOLOv6 architecture enhanced with the GaterNet, featuring sections like ``Gated Efficient Reparameterizable Backbone (EfficientRep)'', ``Gated Rep-Pan Neck'', and ``Efficient Decoupled Heads'', each engineered for maximized performance and efficiency.}
    \label{fig:yolo_architecture}
    \end{figure}

\begin{figure}[ht]
    \centering
    \includesvg[width=\textwidth]{./figures/gated_convolution}
    \caption{Illustration of the Gater Module enhanced with the GaterNet, featuring the duality of paths that entails the completely gated layer and the partially gated layer processes.}
    \label{fig:gating_mechanism}
    \end{figure}

The incorporation of GaterNet into the YOLO architecture facilitates a more selective and efficient processing approach, primarily through two key components:

\textbf{Gated Efficient Reparameterizable Backbone:} This component forms the core of our feature extraction mechanism. By employing gating mechanisms, the EfficientRep selectively emphasizes critical features while minimizing attention to redundant information. This selectivity ensures that subsequent processing layers focus computational resources on analyzing features of utmost relevance to the detection task at hand.

\textbf{Gated Rep-Pan Neck:} Inspired by the PANet topology and enhanced with Rep blocks for added efficiency, the Gated Rep-Pan Neck dynamically adjusts feature resolution and scale. It optimizes the integration and refinement of features passed from the EfficientRep to the detection heads, playing a vital role in ensuring the accuracy of object detection.

\subsubsection{YOLO Gate Module}
The YOLO Gate Module significantly enhances the YOLO architecture by integrating the dynamic gating functionality, offering a novel approach to managing feature processing both during training and inference phases. Through this module, the network learns to modulate its output by performing element-wise multiplication of the convolutional outputs with the gating signals generated by the GaterNet:

\begin{equation}
\mathbf{G} = [g_1, g_2, \ldots, g_c], \label{eq:G_vector}
\end{equation}

\begin{equation}
g_{\text{closed}} = \frac{\sum_{i=1}^{c} \mathbf{1}(G_i = 0)}{c}, \quad \text{and} \label{eq:g_closed}
\end{equation}

\begin{equation}
g(x) = 
\begin{cases} 
0, & \text{if } g_{\text{closed}} > 0.99, \\
\text{Conv}(x) \odot \mathbf{G}, & \text{otherwise}.
\end{cases} \label{eq:conditional_gating}
\end{equation}

\noindent{}where \(\mathbf{G}\) represents the gate vector, indicating the activation status of corresponding feature maps within the network's layers. The module evaluates the proportion of active gates to decide between bypassing certain convolutional operations or allowing them, thus tailoring the network's processing power to the task's specific requirements. This selective gating mechanism, visualized in Figure \ref{fig:gating_mechanism}, ensures optimal resource allocation during feature propagation, enhancing the model's overall efficiency and effectiveness in real-time object detection tasks.

\subsection{Analysis Step}

The analysis step represents a critical phase in our Gated YOLO methodology, where the model's adaptability and efficiency are fine-tuned for specific scene conditions. Through a detailed analysis process, we evaluate the model's performance over a designated scene for a set duration, allowing us to observe and record the operational status of the gating mechanism in real-time scenarios. This process involves a meticulous examination of the gating decisions made by the GaterNet for each frame processed, focusing on identifying which filters are essential for maintaining detection accuracy and which can be deactivated without compromising performance.

\paragraph{Monitoring Gating Decisions.} The essence of this analysis lies in monitoring the closure rate of each gate across the network, a task accomplished by systematically tracking the activation state of each gate throughout the inference process:

\begin{equation}
\Gamma(t) = \{\gamma_{1}(t), \gamma_{2}(t), \ldots, \gamma_{n}(t)\},
\label{eq:gating_states_time}
\end{equation}

\noindent{}where \(\Gamma(t)\) represents the set of gating states at time \(t\), and \(\gamma_{i}(t)\) indicates the state (active or inactive) of the \(i\)-th gate. By analyzing these states over time, we can identify patterns of gate usage that correlate with specific features or elements within the scene.

To facilitate a clearer understanding and interpretation of the gating decisions, we aggregate these decisions by network sections, as visualized in the following figure:

\begin{figure}[htbp]
\centering
\includesvg[width=\textwidth]{./figures/fixed_grouping_gating_analysis}
\caption{Graphical representation of the distribution of gating states across various sections of the network, highlighting the sections with consistently active (``Always On'') or suppressed (``Always Off'') filters, as well as those that are completely deactivated (``Blocked Layer'').}
\label{fig:gating_analysis}
\end{figure}

\paragraph{Distillation of Static Gating Configuration.} The ultimate objective of this analytical step is to derive a static gating configuration that can be consistently applied to the designated scene and similar scenarios in future inferences. This involves discerning which gates remain predominantly active or inactive throughout the analysis period and categorizing them accordingly:

\begin{equation}
G_{\text{static}} = \text{distill}(\Gamma(t), \, \forall t \in T),
\label{eq:static_gating_configuration}
\end{equation}

\noindent{}where \(G_{\text{static}}\) denotes the derived static gating configuration, and \(T\) is the duration of the analysis period. This static configuration enables the model to bypass the need for dynamic gate computations by the GaterNet in subsequent inferences, significantly reducing computational overhead.

\clearpage

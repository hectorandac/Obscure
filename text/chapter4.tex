\section{Experiments and Results}  

This section details the experimental setup, datasets used, performance metrics, and results obtained from the implementation of our proposed gated neural network architecture. Our approach is compared with baseline models to highlight its effectiveness in various tasks. Our goal behind this experiments is to demonstrate our model's capability of decerning between specific characteristics from the scene in a way that it allows the generation of propper gates, particularly focussed on maintaining accuracy as much as possible while reducing the inference speed.

\subsection{Experimental Setup}

The experiments were conducted on a computational setup consisting of an *th generation Intel i7 processor, 32GB RAM, and an NVIDIA RTX 2080 SUPER GPU. Our model was implemented using PyTorch 1.8. During training our learning rate started from $1e-3$, and a batch size of 64 was used across all experiments unless stated otherwise. The influence of the Gating Loss was modified to achieve the best balance between accuracy and inference speed. Laslty, all the samples used for training had a 480 by 480 spatial dimmension which proved to be sufficient for our use cases.

Additionally, experimentations based on practical implementation were conducted on edge like environments. Especifcially, we implemented our approach and compared with other state of the art approaches on an Nvidia Jetson TX2 development board to evaluate the real world perfomrance of outr model in something resembling a traffic surveillance camera system.

\subsection{Datasets}

Experiments were performed on three benchmark datasets:
\begin{itemize}
    \item \textbf{COCO 2017:} The COCO dataset comprises 80 object categories with over 200,000 labeled images, used for object detection and segmentation tasks.
    \item \textbf{VOC 2012:} The Pascal VOC dataset contains images for classification, detection, and segmentation, encompassing 20 object categories.
    \item \textbf{MNIST:} A dataset of handwritten digits used for image classification, containing 60,000 training images and 10,000 test images.
    \item \textbf{ROAD-SEC:} A mixture of datasets composed of traffic survailance camera with the main purpose of detecting motor vehicles on roads. It contains 26,000 files divided into training, test, and validation samples.
\end{itemize}

\subsection{Performance Metrics}

The models' performance was evaluated using the following metrics:
\begin{itemize}
    \item \textbf{Accuracy:} The proportion of correctly predicted observations to the total observations.
    \item \textbf{mean Average Precision (mAP):} Assesses the model's performance across various Intersection over Union (IoU) thresholds, offering a nuanced view of its detection capabilities.
    \item \textbf{Precision and Recall:} Precision is the ratio of correctly predicted positive observations to the total predicted positives, while recall (sensitivity) measures the ratio of correctly predicted positive observations to all observations in actual class.
    \item \textbf{F1 Score:} The weighted average of Precision and Recall.
\end{itemize}

\subsection{Baseline Models}

Our approach was benchmarked against the following models for performance comparison:
\begin{itemize}
    \item Standard YOLOv5~\cite{ultralytics2021yolov5}, YOLOv6~\cite{li2022yolov6,li2023yolov6} and YOLOX~\cite{ge2021yolox}
    \item Yolo-Fastest~\cite{dog2021dog}
    \item ResNet-50~\cite{he2016deep}
    \item EfficientNet~\cite{tan2019efficientnet}
\end{itemize}

\subsection{Results}

\subsubsection{Image Classification on MNIST}

Our experimental results are significant, showing that our gated architecture achieved an accuracy of 99.2\% on the MNIST test set, thereby exceeding the performance of the well-regarded ResNet-50 model by 0.6\%. This improvement not only attests to the model's efficiency in classifying digit images but also highlights the architectural innovations that contributed to its superior performance.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Object Detection Models on MNIST}
    \label{tab:mnist_model_comparison}
    \begin{tabularx}{\textwidth}{@{}Xccccc@{}}
    \toprule
    Model Variant & Accuracy & mAP@0.50:0.95 & Precision & Recall & F1 Score \\ 
    \midrule
    \cite{ultralytics2021yolov5}YOLOv5 N & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 S & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 M & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 L & - & - & - & - & - \\
    \addlinespace
    \cite{li2023yolov6}YOLOv6 N & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 S & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 M & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 L & - & - & - & - & - \\
    \addlinespace
    \cite{ge2021yolox}YOLOX N & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX S & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX M & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX L & - & - & - & - & - \\
    \addlinespace
    \cite{dog2021dog}Yolo-Fastest & - & - & - & - & - \\
    \cite{he2016deep}ResNet-50 & - & - & - & - & - \\
    \cite{tan2019efficientnet}EfficientNet & - & - & - & - & - \\
    \addlinespace
    G-YOLO N & - & - & - & - & - \\
    G-YOLO S & - & - & - & - & - \\
    G-YOLO M & - & - & - & - & - \\
    G-YOLO L & - & - & - & - & - \\
    \bottomrule
    \end{tabularx}
\end{table}


\subsubsection{Object Detection on COCO}

On the COCO dataset, our model demonstrated an improvement in mAP by 2.3\% over the standard YOLOv3. Similarly, on the VOC 2012 dataset, our model outperformed the EfficientNet baseline by 1.8\% in mAP.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Object Detection Models on COCO}
    \label{tab:coco_model_comparison}
    \begin{tabularx}{\textwidth}{@{}Xccccc@{}}
    \toprule
    Model Variant & Accuracy & mAP@0.50:0.95 & Precision & Recall & F1 Score \\ 
    \midrule
    \cite{ultralytics2021yolov5}YOLOv5 N & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 S & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 M & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 L & - & - & - & - & - \\
    \addlinespace
    \cite{li2023yolov6}YOLOv6 N & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 S & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 M & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 L & - & - & - & - & - \\
    \addlinespace
    \cite{ge2021yolox}YOLOX N & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX S & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX M & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX L & - & - & - & - & - \\
    \addlinespace
    \cite{dog2021dog}Yolo-Fastest & - & - & - & - & - \\
    \cite{he2016deep}ResNet-50 & - & - & - & - & - \\
    \cite{tan2019efficientnet}EfficientNet & - & - & - & - & - \\
    \addlinespace
    G-YOLO N & - & - & - & - & - \\
    G-YOLO S & - & - & - & - & - \\
    G-YOLO M & - & - & - & - & - \\
    G-YOLO L & - & - & - & - & - \\
    \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Object Detection on VOC}

On the VOC dataset, our model demonstrated an improvement in mAP by 2.3\% over the standard YOLOv3. Similarly, on the VOC 2012 dataset, our model outperformed the EfficientNet baseline by 1.8\% in mAP.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Object Detection Models on VOC}
    \label{tab:voc_model_comparison}
    \begin{tabularx}{\textwidth}{@{}Xccccc@{}}
    \toprule
    Model Variant & Accuracy & mAP@0.50:0.95 & Precision & Recall & F1 Score \\ 
    \midrule
    \cite{ultralytics2021yolov5}YOLOv5 N & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 S & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 M & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 L & - & - & - & - & - \\
    \addlinespace
    \cite{li2023yolov6}YOLOv6 N & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 S & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 M & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 L & - & - & - & - & - \\
    \addlinespace
    \cite{ge2021yolox}YOLOX N & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX S & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX M & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX L & - & - & - & - & - \\
    \addlinespace
    \cite{dog2021dog}Yolo-Fastest & - & - & - & - & - \\
    \cite{he2016deep}ResNet-50 & - & - & - & - & - \\
    \cite{tan2019efficientnet}EfficientNet & - & - & - & - & - \\
    \addlinespace
    G-YOLO N & - & - & - & - & - \\
    G-YOLO S & - & - & - & - & - \\
    G-YOLO M & - & - & - & - & - \\
    G-YOLO L & - & - & - & - & - \\
    \bottomrule
    \end{tabularx}
\end{table}

\subsubsection{Object Detection on ROAD-SEC}

On the VOC dataset, our model demonstrated an improvement in mAP by 2.3\% over the standard YOLOv3. Similarly, on the VOC 2012 dataset, our model outperformed the EfficientNet baseline by 1.8\% in mAP.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Object Detection Models on MNIST}
    \label{tab:mnist_model_comparison}
    \begin{tabularx}{\textwidth}{@{}Xccccc@{}}
    \toprule
    Model Variant & Accuracy & mAP@0.50:0.95 & Precision & Recall & F1 Score \\ 
    \midrule
    \cite{ultralytics2021yolov5}YOLOv5 N & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 S & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 M & - & - & - & - & - \\
    \cite{ultralytics2021yolov5}YOLOv5 L & - & - & - & - & - \\
    \addlinespace
    \cite{li2023yolov6}YOLOv6 N & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 S & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 M & - & - & - & - & - \\
    \cite{li2023yolov6}YOLOv6 L & - & - & - & - & - \\
    \addlinespace
    \cite{ge2021yolox}YOLOX N & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX S & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX M & - & - & - & - & - \\
    \cite{ge2021yolox}YOLOX L & - & - & - & - & - \\
    \addlinespace
    \cite{dog2021dog}Yolo-Fastest & - & - & - & - & - \\
    \cite{he2016deep}ResNet-50 & - & - & - & - & - \\
    \cite{tan2019efficientnet}EfficientNet & - & - & - & - & - \\
    \addlinespace
    G-YOLO N & - & - & - & - & - \\
    G-YOLO S & - & - & - & - & - \\
    G-YOLO M & - & - & - & - & - \\
    G-YOLO L & - & - & - & - & - \\
    \bottomrule
    \end{tabularx}
\end{table}

\subsection{Ablation Study}

An ablation study was conducted to understand the impact of gating mechanisms in our network. Removing the gating module resulted in a decrease in performance across all datasets, confirming the efficacy of the gating strategy in improving model robustness and accuracy.

\subsubsection{Noise Distribution Strategy}

Exploring the impact of different noise distribution strategies on model performance, this part of the ablation study evaluates how variations in the applied noise affect the gating mechanism's effectiveness. We experimented with Gaussian, Uniform, and no noise scenarios to observe their influence on the overall model accuracy and detection precision.

\begin{table}[ht]
\centering
\caption{Impact of Noise Distribution Strategies on Model Performance}
\label{tab:noise_distribution}
\begin{tabular}{@{}lccc@{}}
\toprule
Noise Strategy & Accuracy (\%) & mAP@0.5 (\%) & F1 Score (\%) \\ 
\midrule
Gaussian & - & - & - \\
Uniform & - & - & - \\
None & - & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Feature Extractor}

This section analyzes the performance variations when employing different feature extractors within the gated architecture. By integrating ResNet-50, MobileNet, and EfficientNet as the backbone for feature extraction, we aim to discern the optimal combination that maximizes both accuracy and efficiency.

\begin{table}[ht]
\centering
\caption{Comparison of Feature Extractors in Gated Architecture}
\label{tab:feature_extractor}
\begin{tabular}{@{}lccc@{}}
\toprule
Feature Extractor & Accuracy (\%) & mAP@0.5 (\%) & F1 Score (\%) \\ 
\midrule
ResNet-50 & - & - & - \\
MobileNet & - & - & - \\
EfficientNet & - & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Alternative YOLO Architecture}

Investigating the adaptability of the gating mechanism across different YOLO architectures, this study contrasts the performance of our gating-enhanced YOLO variants against standard YOLOv5, YOLOv6, and YOLOX models. The focus is on determining whether the inclusion of the gating mechanism universally improves detection capabilities across various YOLO frameworks.

\begin{table}[ht]
\centering
\caption{Performance of Gating Mechanism Across YOLO Architectures}
\label{tab:yolo_architecture}
\begin{tabular}{@{}lccc@{}}
\toprule
YOLO Variant & Accuracy (\%) & mAP@0.5 (\%) & F1 Score (\%) \\ 
\midrule
YOLOv5 + Gating & - & - & - \\
YOLOv6 + Gating & - & - & - \\
YOLOX + Gating & - & - & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion}

The results indicate that the proposed gating mechanism significantly enhances model performance, particularly in scenarios requiring fine-grained feature selection and adaptation. The improvement in mAP on COCO and VOC datasets underscores the potential of gated networks in object detection tasks. Furthermore, the increase in accuracy on MNIST highlights the versatility of the gating mechanism in handling different types of data and tasks.

\clearpage
